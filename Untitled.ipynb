{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4514f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConvexNN.factory import generate_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2889996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Noisy_MNIST import create_dataset, ChainedNoisyMNIST, initialize_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5563358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = initialize_dataset(std=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d784e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonProjects\\ECE-285-Project\\ConvexNN\\train.py:60: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/500] train loss: 41613.22321428572 test loss: 70023.52051717252\n",
      "[2/500] train loss: 77367.01159474206 test loss: nan\n",
      "[3/500] train loss: 78052.13448660714 test loss: nan\n",
      "[4/500] train loss: 78342.64260912698 test loss: nan\n",
      "[5/500] train loss: 76003.27411954365 test loss: nan\n",
      "[6/500] train loss: 77120.63690476191 test loss: nan\n",
      "[7/500] train loss: 78259.2671750992 test loss: nan\n",
      "[8/500] train loss: 86923.51779513889 test loss: nan\n",
      "[9/500] train loss: 74231.4820188492 test loss: nan\n",
      "[10/500] train loss: 79609.00471230158 test loss: nan\n",
      "[11/500] train loss: 73365.06436011905 test loss: 95510.09426168131\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[12/500] train loss: 79583.31429811507 test loss: nan\n",
      "[13/500] train loss: 42356.180927579364 test loss: nan\n",
      "[14/500] train loss: 25954.868567088295 test loss: nan\n",
      "[15/500] train loss: 22369.082186259922 test loss: nan\n",
      "[16/500] train loss: 22916.83306981647 test loss: nan\n",
      "[17/500] train loss: 21910.042410714286 test loss: nan\n",
      "[18/500] train loss: 21854.51667906746 test loss: nan\n",
      "[19/500] train loss: 22144.02584015377 test loss: nan\n",
      "[20/500] train loss: 22506.107096354168 test loss: nan\n",
      "[21/500] train loss: 21497.411551339286 test loss: 24470.213271265973\n",
      "[22/500] train loss: 22429.170030381945 test loss: nan\n",
      "[23/500] train loss: 23108.037062872023 test loss: nan\n",
      "[24/500] train loss: 22196.310500372023 test loss: nan\n",
      "[25/500] train loss: 22360.42720734127 test loss: nan\n",
      "[26/500] train loss: 21269.786287822422 test loss: nan\n",
      "[27/500] train loss: 20998.54422433036 test loss: nan\n",
      "[28/500] train loss: 21555.776475694445 test loss: nan\n",
      "[29/500] train loss: 21706.24420262897 test loss: nan\n",
      "[30/500] train loss: 22017.729988219246 test loss: nan\n",
      "[31/500] train loss: 22212.142624627977 test loss: 24121.71700279553\n",
      "[32/500] train loss: 21149.04296875 test loss: nan\n",
      "[33/500] train loss: 21668.614304315477 test loss: nan\n",
      "[34/500] train loss: 22731.062158978173 test loss: nan\n",
      "[35/500] train loss: 21589.366722470237 test loss: nan\n",
      "[36/500] train loss: 22898.71740141369 test loss: nan\n",
      "[37/500] train loss: 21888.416558159723 test loss: nan\n",
      "Epoch 00038: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[38/500] train loss: 21859.70365203373 test loss: nan\n",
      "[39/500] train loss: 13270.53755890377 test loss: nan\n",
      "[40/500] train loss: 8199.849927145337 test loss: nan\n",
      "[41/500] train loss: 8053.318343874008 test loss: 8013.922815682408\n",
      "[42/500] train loss: 7906.58683655754 test loss: nan\n",
      "[43/500] train loss: 8002.602306547619 test loss: nan\n",
      "[44/500] train loss: 7922.208542596726 test loss: nan\n",
      "[45/500] train loss: 8093.744512648809 test loss: nan\n",
      "[46/500] train loss: 7846.332418774801 test loss: nan\n",
      "[47/500] train loss: 8056.0636974516365 test loss: nan\n",
      "[48/500] train loss: 8135.543329148066 test loss: nan\n",
      "[49/500] train loss: 7889.684213789683 test loss: nan\n",
      "[50/500] train loss: 7909.980422247024 test loss: nan\n",
      "[51/500] train loss: 7888.596761067708 test loss: 8794.34986365565\n",
      "[52/500] train loss: 7931.494113498264 test loss: nan\n",
      "[53/500] train loss: 7986.648646763393 test loss: nan\n",
      "[54/500] train loss: 7787.007107204861 test loss: nan\n",
      "[55/500] train loss: 7830.839130704365 test loss: nan\n",
      "[56/500] train loss: 8064.524045913939 test loss: nan\n",
      "[57/500] train loss: 8278.373643663195 test loss: nan\n",
      "[58/500] train loss: 7939.068591889881 test loss: nan\n",
      "[59/500] train loss: 7889.846966455853 test loss: nan\n",
      "[60/500] train loss: 7972.066208612351 test loss: nan\n",
      "[61/500] train loss: 7798.24421812996 test loss: 8439.57424995008\n",
      "[62/500] train loss: 7909.15867203001 test loss: nan\n",
      "[63/500] train loss: 8021.886858258928 test loss: nan\n",
      "[64/500] train loss: 8048.784524584574 test loss: nan\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[65/500] train loss: 7957.360971602183 test loss: nan\n",
      "[66/500] train loss: 5419.289252387152 test loss: nan\n",
      "[67/500] train loss: 4148.520901150174 test loss: nan\n",
      "[68/500] train loss: 4065.499455527654 test loss: nan\n",
      "[69/500] train loss: 4016.7128169952875 test loss: nan\n",
      "[70/500] train loss: 4038.2005789620534 test loss: nan\n",
      "[71/500] train loss: 4013.806710379464 test loss: 4113.225720097844\n",
      "[72/500] train loss: 4034.0604073660716 test loss: nan\n",
      "[73/500] train loss: 4032.7906552269346 test loss: nan\n",
      "[74/500] train loss: 3982.2719842819943 test loss: nan\n",
      "[75/500] train loss: 4033.127511160714 test loss: nan\n",
      "[76/500] train loss: 4043.940483940972 test loss: nan\n",
      "[77/500] train loss: 3973.1167108444943 test loss: nan\n",
      "[78/500] train loss: 3976.907524956597 test loss: nan\n",
      "[79/500] train loss: 4016.7308020213295 test loss: nan\n",
      "[80/500] train loss: 4049.6947157118057 test loss: nan\n",
      "[81/500] train loss: 4014.3437984406 test loss: 4007.5130884335063\n",
      "[82/500] train loss: 3980.6744752914187 test loss: nan\n",
      "[83/500] train loss: 4016.5982181609625 test loss: nan\n",
      "[84/500] train loss: 4034.1783854166665 test loss: nan\n",
      "[85/500] train loss: 3968.7316080729165 test loss: nan\n",
      "[86/500] train loss: 4063.9302687872023 test loss: nan\n",
      "[87/500] train loss: 4012.0816165984625 test loss: nan\n",
      "[88/500] train loss: 3989.3014284164187 test loss: nan\n",
      "[89/500] train loss: 4047.0065065414187 test loss: nan\n",
      "[90/500] train loss: 4013.0817096044148 test loss: nan\n",
      "[91/500] train loss: 3985.105242047991 test loss: 4125.235167482028\n",
      "[92/500] train loss: 4023.6888485863096 test loss: nan\n",
      "[93/500] train loss: 4035.152824280754 test loss: nan\n",
      "[94/500] train loss: 3983.4098365420386 test loss: nan\n",
      "[95/500] train loss: 3993.4346187531 test loss: nan\n",
      "Epoch 00096: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[96/500] train loss: 4038.9067034040177 test loss: nan\n",
      "[97/500] train loss: 3141.913283575149 test loss: nan\n",
      "[98/500] train loss: 2727.110911535838 test loss: nan\n",
      "[99/500] train loss: 2699.799291217138 test loss: nan\n",
      "[100/500] train loss: 2676.2728969029017 test loss: nan\n",
      "[101/500] train loss: 2670.7603314112102 test loss: 2672.0516509522263\n",
      "[102/500] train loss: 2658.0842943948414 test loss: nan\n",
      "[103/500] train loss: 2663.2339370969744 test loss: nan\n",
      "[104/500] train loss: 2669.0202985491073 test loss: nan\n",
      "[105/500] train loss: 2658.6294720362102 test loss: nan\n",
      "[106/500] train loss: 2683.2118249317955 test loss: nan\n",
      "[107/500] train loss: 2657.578669472346 test loss: nan\n",
      "[108/500] train loss: 2679.783542209201 test loss: nan\n",
      "[109/500] train loss: 2676.4454675099205 test loss: nan\n",
      "[110/500] train loss: 2665.063468812004 test loss: nan\n",
      "[111/500] train loss: 2667.2481476314483 test loss: 2695.149357122354\n",
      "[112/500] train loss: 2650.9419526599704 test loss: nan\n",
      "[113/500] train loss: 2672.0517926897323 test loss: nan\n",
      "[114/500] train loss: 2674.742472330729 test loss: nan\n",
      "[115/500] train loss: 2670.992423890129 test loss: nan\n",
      "[116/500] train loss: 2664.1424444289432 test loss: nan\n",
      "[117/500] train loss: 2679.1542309957836 test loss: nan\n",
      "[118/500] train loss: 2677.429885137649 test loss: nan\n",
      "[119/500] train loss: 2686.340992761037 test loss: nan\n",
      "[120/500] train loss: 2680.86319405692 test loss: nan\n",
      "[121/500] train loss: 2675.0140632750495 test loss: 2663.1817907753843\n",
      "[122/500] train loss: 2667.2724725632443 test loss: nan\n",
      "Epoch 00123: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[123/500] train loss: 2671.688397119916 test loss: nan\n",
      "[124/500] train loss: 2346.5708782862102 test loss: nan\n",
      "[125/500] train loss: 2182.388177780878 test loss: nan\n",
      "[126/500] train loss: 2138.697779095362 test loss: nan\n",
      "[127/500] train loss: 2128.277853345114 test loss: nan\n",
      "[128/500] train loss: 2135.0843583364335 test loss: nan\n",
      "[129/500] train loss: 2123.3582996186756 test loss: nan\n",
      "[130/500] train loss: 2130.0667511470733 test loss: nan\n",
      "[131/500] train loss: 2120.2810882083954 test loss: 2182.87741020617\n",
      "[132/500] train loss: 2125.710668170263 test loss: nan\n",
      "[133/500] train loss: 2121.2574540395585 test loss: nan\n",
      "[134/500] train loss: 2115.2598005022323 test loss: nan\n",
      "[135/500] train loss: 2131.008546859499 test loss: nan\n",
      "[136/500] train loss: 2119.2249465215773 test loss: nan\n",
      "[137/500] train loss: 2120.4070589580233 test loss: nan\n",
      "[138/500] train loss: 2128.724843827505 test loss: nan\n",
      "[139/500] train loss: 2116.0558636377727 test loss: nan\n",
      "[140/500] train loss: 2120.9599163721477 test loss: nan\n",
      "[141/500] train loss: 2122.552009703621 test loss: 2184.5600390937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142/500] train loss: 2130.055436391679 test loss: nan\n",
      "[143/500] train loss: 2123.6449342757937 test loss: nan\n",
      "[144/500] train loss: 2128.335451156374 test loss: nan\n",
      "Epoch 00145: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[145/500] train loss: 2120.3528607080852 test loss: nan\n",
      "[146/500] train loss: 1994.5114804222471 test loss: nan\n",
      "[147/500] train loss: 1913.0853029281375 test loss: nan\n",
      "[148/500] train loss: 1890.3684508308531 test loss: nan\n",
      "[149/500] train loss: 1882.0867910233756 test loss: nan\n",
      "[150/500] train loss: 1879.7703431144594 test loss: nan\n",
      "[151/500] train loss: 1876.9219650995165 test loss: 1918.330963232266\n",
      "[152/500] train loss: 1864.6236426943824 test loss: nan\n",
      "[153/500] train loss: 1880.2239215184773 test loss: nan\n",
      "[154/500] train loss: 1874.862315344432 test loss: nan\n",
      "[155/500] train loss: 1866.8526349748884 test loss: nan\n",
      "[156/500] train loss: 1868.4662010556176 test loss: nan\n",
      "[157/500] train loss: 1866.7870260571676 test loss: nan\n",
      "[158/500] train loss: 1870.6279703776042 test loss: nan\n",
      "[159/500] train loss: 1875.376709953187 test loss: nan\n",
      "[160/500] train loss: 1868.9913194444443 test loss: nan\n",
      "[161/500] train loss: 1871.217048766121 test loss: 1920.17223828593\n",
      "[162/500] train loss: 1869.604524158296 test loss: nan\n",
      "[163/500] train loss: 1863.946786063058 test loss: nan\n",
      "[164/500] train loss: 1867.3920346214659 test loss: nan\n",
      "[165/500] train loss: 1868.8554668123759 test loss: nan\n",
      "[166/500] train loss: 1868.6148507254463 test loss: nan\n",
      "[167/500] train loss: 1865.5943622891866 test loss: nan\n",
      "[168/500] train loss: 1875.4739215184773 test loss: nan\n",
      "[169/500] train loss: 1872.4302251906622 test loss: nan\n",
      "[170/500] train loss: 1866.7810077969991 test loss: nan\n",
      "[171/500] train loss: 1862.776332310268 test loss: 1919.3931861365566\n",
      "[172/500] train loss: 1860.5901528010293 test loss: nan\n",
      "[173/500] train loss: 1866.773676796565 test loss: nan\n",
      "[174/500] train loss: 1872.672647143167 test loss: nan\n",
      "[175/500] train loss: 1860.687260703435 test loss: nan\n",
      "[176/500] train loss: 1867.1317574637276 test loss: nan\n",
      "[177/500] train loss: 1875.3976236979167 test loss: nan\n",
      "[178/500] train loss: 1866.1837206643725 test loss: nan\n",
      "[179/500] train loss: 1869.0401562887525 test loss: nan\n",
      "[180/500] train loss: 1871.3194657583085 test loss: nan\n",
      "[181/500] train loss: 1867.8827756851438 test loss: 1916.8103750795601\n",
      "[182/500] train loss: 1863.8462417844742 test loss: nan\n",
      "Epoch 00183: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[183/500] train loss: 1868.145524282304 test loss: nan\n",
      "[184/500] train loss: 1812.649186391679 test loss: nan\n",
      "[185/500] train loss: 1785.7471671937003 test loss: nan\n",
      "[186/500] train loss: 1767.971200125558 test loss: nan\n",
      "[187/500] train loss: 1762.6829649910094 test loss: nan\n",
      "[188/500] train loss: 1755.115706186446 test loss: nan\n",
      "[189/500] train loss: 1747.9374040876116 test loss: nan\n",
      "[190/500] train loss: 1747.777564639137 test loss: nan\n",
      "[191/500] train loss: 1749.01025875031 test loss: 1814.4614060861995\n",
      "[192/500] train loss: 1752.9005185081846 test loss: nan\n",
      "[193/500] train loss: 1745.655470106337 test loss: nan\n",
      "[194/500] train loss: 1741.8815075102307 test loss: nan\n",
      "[195/500] train loss: 1740.7820463634673 test loss: nan\n",
      "[196/500] train loss: 1752.3193184988838 test loss: nan\n",
      "[197/500] train loss: 1751.1080370706225 test loss: nan\n",
      "[198/500] train loss: 1745.162822420635 test loss: nan\n",
      "[199/500] train loss: 1740.2206052749875 test loss: nan\n",
      "[200/500] train loss: 1751.9815983847966 test loss: nan\n",
      "[201/500] train loss: 1742.060788109189 test loss: 1810.5649534962809\n",
      "[202/500] train loss: 1744.1251588851687 test loss: nan\n",
      "[203/500] train loss: 1748.4115232437375 test loss: nan\n",
      "[204/500] train loss: 1748.942143515935 test loss: nan\n",
      "[205/500] train loss: 1749.4939739893352 test loss: nan\n",
      "[206/500] train loss: 1742.4818260556176 test loss: nan\n",
      "[207/500] train loss: 1746.9610556950645 test loss: nan\n",
      "[208/500] train loss: 1740.6853908962673 test loss: nan\n",
      "[209/500] train loss: 1745.104741172185 test loss: nan\n",
      "Epoch 00210: reducing learning rate of group 0 to 3.9063e-06.\n",
      "[210/500] train loss: 1755.4601023840526 test loss: nan\n",
      "[211/500] train loss: 1715.7374306330605 test loss: 1786.5067398022538\n",
      "[212/500] train loss: 1707.6958501906622 test loss: nan\n",
      "[213/500] train loss: 1701.8813137478298 test loss: nan\n",
      "[214/500] train loss: 1700.6116778661335 test loss: nan\n",
      "[215/500] train loss: 1695.514161125062 test loss: nan\n",
      "[216/500] train loss: 1692.0388396732392 test loss: nan\n",
      "[217/500] train loss: 1692.210476345486 test loss: nan\n",
      "[218/500] train loss: 1691.4759434291295 test loss: nan\n",
      "[219/500] train loss: 1695.5072980608259 test loss: nan\n",
      "[220/500] train loss: 1691.0703667534722 test loss: nan\n",
      "[221/500] train loss: 1691.4631899879091 test loss: 1762.9689307654628\n",
      "[222/500] train loss: 1682.3267841641866 test loss: nan\n",
      "[223/500] train loss: 1689.3389679439483 test loss: nan\n",
      "[224/500] train loss: 1686.7760842943949 test loss: nan\n",
      "[225/500] train loss: 1685.0908697219122 test loss: nan\n",
      "[226/500] train loss: 1690.297619047619 test loss: nan\n",
      "[227/500] train loss: 1688.4713435097347 test loss: nan\n",
      "[228/500] train loss: 1681.040008060516 test loss: nan\n",
      "[229/500] train loss: 1685.189194452195 test loss: nan\n",
      "[230/500] train loss: 1689.4608222113716 test loss: nan\n",
      "[231/500] train loss: 1686.2627980065724 test loss: 1761.0280553068217\n",
      "[232/500] train loss: 1691.2227696010045 test loss: nan\n",
      "[233/500] train loss: 1685.968221997458 test loss: nan\n",
      "[234/500] train loss: 1679.9420534164187 test loss: nan\n",
      "[235/500] train loss: 1677.6419358026415 test loss: nan\n",
      "[236/500] train loss: 1683.9641297355531 test loss: nan\n",
      "[237/500] train loss: 1679.986851283482 test loss: nan\n",
      "[238/500] train loss: 1684.9521620008682 test loss: nan\n",
      "[239/500] train loss: 1685.110598609561 test loss: nan\n",
      "[240/500] train loss: 1684.8592316158233 test loss: nan\n",
      "[241/500] train loss: 1686.5746004619295 test loss: 1759.1699985101961\n",
      "[242/500] train loss: 1683.8506663488963 test loss: nan\n",
      "[243/500] train loss: 1685.3301546611483 test loss: nan\n",
      "[244/500] train loss: 1695.0433427114335 test loss: nan\n",
      "[245/500] train loss: 1683.4554026770213 test loss: nan\n",
      "Epoch 00246: reducing learning rate of group 0 to 1.9531e-06.\n",
      "[246/500] train loss: 1681.452163938492 test loss: nan\n",
      "[247/500] train loss: 1670.551978701637 test loss: nan\n",
      "[248/500] train loss: 1664.353968060206 test loss: nan\n",
      "[249/500] train loss: 1662.7617691282242 test loss: nan\n",
      "[250/500] train loss: 1666.4383990575398 test loss: nan\n",
      "[251/500] train loss: 1664.233168829055 test loss: 1743.2743580973568\n",
      "[252/500] train loss: 1662.4185015966023 test loss: nan\n",
      "[253/500] train loss: 1659.8424847315227 test loss: nan\n",
      "[254/500] train loss: 1653.2644633944071 test loss: nan\n",
      "[255/500] train loss: 1658.9056599934895 test loss: nan\n",
      "[256/500] train loss: 1654.6903386191716 test loss: nan\n",
      "[257/500] train loss: 1651.450936453683 test loss: nan\n",
      "[258/500] train loss: 1658.2727583627852 test loss: nan\n",
      "[259/500] train loss: 1659.1222117590526 test loss: nan\n",
      "[260/500] train loss: 1665.669481065538 test loss: nan\n",
      "[261/500] train loss: 1660.1471761067708 test loss: 1736.4077823139228\n",
      "[262/500] train loss: 1651.0533253503224 test loss: nan\n",
      "[263/500] train loss: 1654.868411109561 test loss: nan\n",
      "[264/500] train loss: 1650.9229891338045 test loss: nan\n",
      "[265/500] train loss: 1659.1292976500497 test loss: nan\n",
      "[266/500] train loss: 1654.4265601748511 test loss: nan\n",
      "[267/500] train loss: 1650.1444644019716 test loss: nan\n",
      "[268/500] train loss: 1657.6604498000372 test loss: nan\n",
      "[269/500] train loss: 1655.9087456597222 test loss: nan\n",
      "[270/500] train loss: 1650.9334988064236 test loss: nan\n",
      "[271/500] train loss: 1654.5989631773934 test loss: 1736.6096715957592\n",
      "[272/500] train loss: 1655.4577045743429 test loss: nan\n",
      "[273/500] train loss: 1658.0182659815227 test loss: nan\n",
      "[274/500] train loss: 1652.2145017593625 test loss: nan\n",
      "[275/500] train loss: 1656.6616656591023 test loss: nan\n",
      "[276/500] train loss: 1649.7827652219742 test loss: nan\n",
      "[277/500] train loss: 1653.1538696289062 test loss: nan\n",
      "[278/500] train loss: 1653.266313825335 test loss: nan\n",
      "[279/500] train loss: 1650.0057305230034 test loss: nan\n",
      "[280/500] train loss: 1652.0543784489707 test loss: nan\n",
      "[281/500] train loss: 1653.0178774879091 test loss: 1736.233997674034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[282/500] train loss: 1650.5406687903026 test loss: nan\n",
      "[283/500] train loss: 1653.0576268756201 test loss: nan\n",
      "[284/500] train loss: 1653.544177827381 test loss: nan\n",
      "[285/500] train loss: 1652.1073133680557 test loss: nan\n",
      "[286/500] train loss: 1652.8534071180557 test loss: nan\n",
      "Epoch 00287: reducing learning rate of group 0 to 9.7656e-07.\n",
      "[287/500] train loss: 1657.0513693188864 test loss: nan\n",
      "[288/500] train loss: 1647.6099834139384 test loss: nan\n",
      "[289/500] train loss: 1645.6146182105654 test loss: nan\n",
      "[290/500] train loss: 1650.222903297061 test loss: nan\n",
      "[291/500] train loss: 1647.298072451637 test loss: 1729.7466454064122\n",
      "[292/500] train loss: 1643.3285280257937 test loss: nan\n",
      "[293/500] train loss: 1638.3376135447668 test loss: nan\n",
      "[294/500] train loss: 1647.9608745272196 test loss: nan\n",
      "[295/500] train loss: 1636.8460518973213 test loss: nan\n",
      "[296/500] train loss: 1645.1103457496279 test loss: nan\n",
      "[297/500] train loss: 1642.6101597377233 test loss: nan\n",
      "[298/500] train loss: 1640.7552790566097 test loss: nan\n",
      "[299/500] train loss: 1645.8821382068452 test loss: nan\n",
      "[300/500] train loss: 1641.9102996341767 test loss: nan\n",
      "[301/500] train loss: 1642.959954155816 test loss: 1726.7840192020892\n",
      "[302/500] train loss: 1649.9454229445685 test loss: nan\n",
      "[303/500] train loss: 1641.7344050331722 test loss: nan\n",
      "[304/500] train loss: 1644.5147850399926 test loss: nan\n",
      "[305/500] train loss: 1637.2689160543775 test loss: nan\n",
      "[306/500] train loss: 1635.162101624504 test loss: nan\n",
      "[307/500] train loss: 1646.8237721276662 test loss: nan\n",
      "[308/500] train loss: 1644.2333228701636 test loss: nan\n",
      "[309/500] train loss: 1641.351105220734 test loss: nan\n",
      "[310/500] train loss: 1638.885729592944 test loss: nan\n",
      "[311/500] train loss: 1642.7625228639633 test loss: 1723.5557676077651\n",
      "[312/500] train loss: 1643.1795160202753 test loss: nan\n",
      "[313/500] train loss: 1635.1517343672494 test loss: nan\n",
      "[314/500] train loss: 1639.137655591208 test loss: nan\n",
      "[315/500] train loss: 1642.4103190104167 test loss: nan\n",
      "[316/500] train loss: 1639.5977308485244 test loss: nan\n",
      "Epoch 00317: reducing learning rate of group 0 to 4.8828e-07.\n",
      "[317/500] train loss: 1641.452161032056 test loss: nan\n",
      "[318/500] train loss: 1632.932344951327 test loss: nan\n",
      "[319/500] train loss: 1638.748299734933 test loss: nan\n",
      "[320/500] train loss: 1629.5753144763764 test loss: nan\n",
      "[321/500] train loss: 1632.8208104693701 test loss: 1719.4536320012978\n",
      "[322/500] train loss: 1630.9505838061136 test loss: nan\n",
      "[323/500] train loss: 1634.157505580357 test loss: nan\n",
      "[324/500] train loss: 1633.6820552765378 test loss: nan\n",
      "[325/500] train loss: 1629.6722877139136 test loss: nan\n",
      "[326/500] train loss: 1628.9948827349951 test loss: nan\n",
      "[327/500] train loss: 1637.7138449048239 test loss: nan\n",
      "[328/500] train loss: 1638.305388919891 test loss: nan\n",
      "[329/500] train loss: 1635.2437928214906 test loss: nan\n",
      "[330/500] train loss: 1632.6539519779267 test loss: nan\n",
      "[331/500] train loss: 1635.088174486917 test loss: 1719.9606449992511\n",
      "[332/500] train loss: 1627.5615796285963 test loss: nan\n",
      "[333/500] train loss: 1628.199446420821 test loss: nan\n",
      "[334/500] train loss: 1636.9075298006571 test loss: nan\n",
      "[335/500] train loss: 1637.2010934012276 test loss: nan\n",
      "[336/500] train loss: 1634.0571967230903 test loss: nan\n",
      "[337/500] train loss: 1631.5594511486236 test loss: nan\n",
      "[338/500] train loss: 1631.8445996481275 test loss: nan\n",
      "[339/500] train loss: 1628.6458953373017 test loss: nan\n",
      "[340/500] train loss: 1630.3732280428446 test loss: nan\n",
      "[341/500] train loss: 1631.5079336015006 test loss: 1716.1217154115914\n",
      "[342/500] train loss: 1634.3665442088293 test loss: nan\n",
      "Epoch 00343: reducing learning rate of group 0 to 2.4414e-07.\n",
      "[343/500] train loss: 1629.9298347594247 test loss: nan\n",
      "[344/500] train loss: 1631.6342230902778 test loss: nan\n",
      "[345/500] train loss: 1631.565217517671 test loss: nan\n",
      "[346/500] train loss: 1631.5740375821554 test loss: nan\n",
      "[347/500] train loss: 1633.468721904452 test loss: nan\n",
      "[348/500] train loss: 1629.902126736111 test loss: nan\n",
      "[349/500] train loss: 1629.4468500046503 test loss: nan\n",
      "[350/500] train loss: 1632.806671626984 test loss: nan\n",
      "[351/500] train loss: 1632.0132572234622 test loss: 1716.3452770489093\n",
      "[352/500] train loss: 1631.6737554640997 test loss: nan\n",
      "[353/500] train loss: 1629.4114728655134 test loss: nan\n",
      "Epoch 00354: reducing learning rate of group 0 to 1.2207e-07.\n",
      "[354/500] train loss: 1629.4939672076514 test loss: nan\n",
      "[355/500] train loss: 1622.381068638393 test loss: nan\n",
      "[356/500] train loss: 1635.4213043697296 test loss: nan\n",
      "[357/500] train loss: 1626.741227407304 test loss: nan\n",
      "[358/500] train loss: 1629.6172524104043 test loss: nan\n",
      "[359/500] train loss: 1633.384237622458 test loss: nan\n",
      "[360/500] train loss: 1633.265105716766 test loss: nan\n",
      "[361/500] train loss: 1623.9169301835318 test loss: 1717.8282956254368\n",
      "[362/500] train loss: 1626.8880014570932 test loss: nan\n",
      "[363/500] train loss: 1631.5662212069071 test loss: nan\n",
      "[364/500] train loss: 1627.551030234685 test loss: nan\n",
      "[365/500] train loss: 1624.6075739784847 test loss: nan\n",
      "Epoch 00366: reducing learning rate of group 0 to 6.1035e-08.\n",
      "[366/500] train loss: 1628.159719315786 test loss: nan\n",
      "[367/500] train loss: 1630.3226570250497 test loss: nan\n",
      "[368/500] train loss: 1624.6291019500247 test loss: nan\n",
      "[369/500] train loss: 1632.018547905816 test loss: nan\n",
      "[370/500] train loss: 1628.2309512183779 test loss: nan\n",
      "[371/500] train loss: 1631.5338638547867 test loss: 1718.9157164942342\n",
      "[372/500] train loss: 1629.7545040070065 test loss: nan\n",
      "[373/500] train loss: 1629.0455612909227 test loss: nan\n",
      "[374/500] train loss: 1626.4280879913815 test loss: nan\n",
      "[375/500] train loss: 1630.667031908792 test loss: nan\n",
      "[376/500] train loss: 1623.4039994497148 test loss: nan\n",
      "Epoch 00377: reducing learning rate of group 0 to 3.0518e-08.\n",
      "[377/500] train loss: 1631.274895562066 test loss: nan\n",
      "[378/500] train loss: 1630.562696668837 test loss: nan\n",
      "[379/500] train loss: 1633.0784669906375 test loss: nan\n",
      "[380/500] train loss: 1625.3355674138145 test loss: nan\n",
      "[381/500] train loss: 1628.7930171906003 test loss: 1715.0748950117313\n",
      "[382/500] train loss: 1624.703614250062 test loss: nan\n",
      "[383/500] train loss: 1624.1942041790674 test loss: nan\n",
      "[384/500] train loss: 1628.170126294333 test loss: nan\n",
      "[385/500] train loss: 1622.8821672712054 test loss: nan\n",
      "[386/500] train loss: 1628.3153841533358 test loss: nan\n",
      "[387/500] train loss: 1629.1608072916667 test loss: nan\n",
      "Epoch 00388: reducing learning rate of group 0 to 1.5259e-08.\n",
      "[388/500] train loss: 1625.906226748512 test loss: nan\n",
      "[389/500] train loss: 1629.4246031746031 test loss: nan\n",
      "[390/500] train loss: 1626.2618059430804 test loss: nan\n",
      "[391/500] train loss: 1627.8781186058409 test loss: 1717.6361865936376\n",
      "[392/500] train loss: 1629.3871149941097 test loss: nan\n",
      "[393/500] train loss: 1628.7144339425224 test loss: nan\n",
      "[394/500] train loss: 1630.3936874147446 test loss: nan\n",
      "[395/500] train loss: 1627.2132335844494 test loss: nan\n",
      "[396/500] train loss: 1627.7654554578994 test loss: nan\n",
      "[397/500] train loss: 1620.7874784923736 test loss: nan\n",
      "[398/500] train loss: 1634.0547814747645 test loss: nan\n",
      "[399/500] train loss: 1626.452649313306 test loss: nan\n",
      "[400/500] train loss: 1630.1346416170634 test loss: nan\n",
      "[401/500] train loss: 1621.2189263237847 test loss: 1716.575769198969\n",
      "[402/500] train loss: 1632.7352333674355 test loss: nan\n",
      "[403/500] train loss: 1624.2271360367063 test loss: nan\n",
      "[404/500] train loss: 1625.1971522739955 test loss: nan\n",
      "[405/500] train loss: 1626.8216194273934 test loss: nan\n",
      "[406/500] train loss: 1630.9515564933656 test loss: nan\n",
      "[407/500] train loss: 1626.498784140935 test loss: nan\n",
      "Epoch 00408: reducing learning rate of group 0 to 7.6294e-09.\n",
      "[408/500] train loss: 1626.2854498000372 test loss: nan\n",
      "[409/500] train loss: 1627.8421504913815 test loss: nan\n",
      "[410/500] train loss: 1629.1766434926835 test loss: nan\n",
      "[411/500] train loss: 1628.2980346679688 test loss: 1717.0705494256065\n",
      "[412/500] train loss: 1628.6712694924975 test loss: nan\n",
      "[413/500] train loss: 1622.9198685903398 test loss: nan\n",
      "[414/500] train loss: 1628.3924386160713 test loss: nan\n",
      "[415/500] train loss: 1628.4964919317335 test loss: nan\n",
      "[416/500] train loss: 1627.7593645368304 test loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[417/500] train loss: 1628.956791953435 test loss: nan\n",
      "[418/500] train loss: 1625.2189127604167 test loss: nan\n",
      "Epoch 00419: reducing learning rate of group 0 to 3.8147e-09.\n",
      "[419/500] train loss: 1626.3179951016866 test loss: nan\n",
      "[420/500] train loss: 1624.0859791589162 test loss: nan\n",
      "[421/500] train loss: 1622.7537551153273 test loss: 1717.0215701752197\n",
      "[422/500] train loss: 1630.1367720346602 test loss: nan\n",
      "[423/500] train loss: 1626.7587716238838 test loss: nan\n",
      "[424/500] train loss: 1628.552273220486 test loss: nan\n",
      "[425/500] train loss: 1624.1607133169023 test loss: nan\n",
      "[426/500] train loss: 1624.8638799758185 test loss: nan\n",
      "[427/500] train loss: 1629.8023507254463 test loss: nan\n",
      "[428/500] train loss: 1625.9190867590526 test loss: nan\n",
      "[429/500] train loss: 1632.798063732329 test loss: nan\n",
      "Epoch 00430: reducing learning rate of group 0 to 1.9073e-09.\n",
      "[430/500] train loss: 1628.1181020585318 test loss: nan\n",
      "[431/500] train loss: 1628.8254103887648 test loss: 1716.1589006417857\n",
      "[432/500] train loss: 1627.4982609824528 test loss: nan\n",
      "[433/500] train loss: 1638.6160946800596 test loss: nan\n",
      "[434/500] train loss: 1622.1102178664435 test loss: nan\n",
      "[435/500] train loss: 1629.6167476593503 test loss: nan\n",
      "[436/500] train loss: 1618.134280250186 test loss: nan\n",
      "[437/500] train loss: 1636.7532116117932 test loss: nan\n",
      "[438/500] train loss: 1630.638218470982 test loss: nan\n",
      "[439/500] train loss: 1625.05612812345 test loss: nan\n",
      "[440/500] train loss: 1620.9308374798486 test loss: nan\n",
      "[441/500] train loss: 1627.5545993381077 test loss: 1717.0944670168356\n",
      "[442/500] train loss: 1625.6680656312003 test loss: nan\n",
      "[443/500] train loss: 1627.2477969215029 test loss: nan\n",
      "[444/500] train loss: 1632.386911543589 test loss: nan\n",
      "[445/500] train loss: 1621.3106573195685 test loss: nan\n",
      "[446/500] train loss: 1627.1927083333333 test loss: nan\n",
      "Epoch 00447: reducing learning rate of group 0 to 9.5367e-10.\n",
      "[447/500] train loss: 1630.0135158962673 test loss: nan\n",
      "[448/500] train loss: 1623.6234343998017 test loss: nan\n",
      "[449/500] train loss: 1626.56836421906 test loss: nan\n",
      "[450/500] train loss: 1621.7361915225074 test loss: nan\n",
      "[451/500] train loss: 1628.6831558469742 test loss: 1716.1645435662315\n",
      "[452/500] train loss: 1626.1534336635045 test loss: nan\n",
      "[453/500] train loss: 1625.5050378224207 test loss: nan\n",
      "[454/500] train loss: 1627.9587150452628 test loss: nan\n",
      "[455/500] train loss: 1626.2390562996031 test loss: nan\n",
      "[456/500] train loss: 1623.7279750279017 test loss: nan\n",
      "[457/500] train loss: 1622.8084639291915 test loss: nan\n",
      "Epoch 00458: reducing learning rate of group 0 to 4.7684e-10.\n",
      "[458/500] train loss: 1621.7886245969742 test loss: nan\n",
      "[459/500] train loss: 1630.4767678881449 test loss: nan\n",
      "[460/500] train loss: 1625.632557702443 test loss: nan\n",
      "[461/500] train loss: 1624.1306878952753 test loss: 1716.3805300191568\n",
      "[462/500] train loss: 1623.5140700567335 test loss: nan\n",
      "[463/500] train loss: 1635.8619966052827 test loss: nan\n",
      "[464/500] train loss: 1622.7207980685764 test loss: nan\n",
      "[465/500] train loss: 1626.7277270120287 test loss: nan\n",
      "[466/500] train loss: 1624.596156529018 test loss: nan\n",
      "[467/500] train loss: 1627.6760835193452 test loss: nan\n",
      "[468/500] train loss: 1624.576447986421 test loss: nan\n",
      "Epoch 00469: reducing learning rate of group 0 to 2.3842e-10.\n",
      "[469/500] train loss: 1624.2363571893602 test loss: nan\n",
      "[470/500] train loss: 1624.3638421921503 test loss: nan\n",
      "[471/500] train loss: 1628.050832597036 test loss: 1716.6793495641348\n",
      "[472/500] train loss: 1627.2294127449156 test loss: nan\n",
      "[473/500] train loss: 1622.4512774755085 test loss: nan\n",
      "[474/500] train loss: 1622.541981530568 test loss: nan\n",
      "[475/500] train loss: 1628.21434093657 test loss: nan\n",
      "[476/500] train loss: 1630.530014764695 test loss: nan\n",
      "[477/500] train loss: 1629.7423987010168 test loss: nan\n",
      "[478/500] train loss: 1627.093726748512 test loss: nan\n",
      "[479/500] train loss: 1627.880583263579 test loss: nan\n",
      "Epoch 00480: reducing learning rate of group 0 to 1.1921e-10.\n",
      "[480/500] train loss: 1629.5487428695437 test loss: nan\n",
      "[481/500] train loss: 1631.1046617296006 test loss: 1715.8609166739466\n",
      "[482/500] train loss: 1625.8785816979787 test loss: nan\n",
      "[483/500] train loss: 1631.8117142934648 test loss: nan\n",
      "[484/500] train loss: 1630.2647898840526 test loss: nan\n",
      "[485/500] train loss: 1629.806143624442 test loss: nan\n",
      "[486/500] train loss: 1629.6819583953372 test loss: nan\n",
      "[487/500] train loss: 1624.3868127247645 test loss: nan\n",
      "[488/500] train loss: 1624.2653866722471 test loss: nan\n",
      "[489/500] train loss: 1628.3082052563864 test loss: nan\n",
      "[490/500] train loss: 1627.244597904266 test loss: nan\n",
      "Epoch 00491: reducing learning rate of group 0 to 5.9605e-11.\n",
      "[491/500] train loss: 1629.511502704923 test loss: 1716.2995619118785\n",
      "[492/500] train loss: 1622.444834875682 test loss: nan\n",
      "[493/500] train loss: 1629.697238498264 test loss: nan\n",
      "[494/500] train loss: 1626.2598266601562 test loss: nan\n",
      "[495/500] train loss: 1622.810519748264 test loss: nan\n",
      "[496/500] train loss: 1626.0385112459696 test loss: nan\n",
      "[497/500] train loss: 1624.3667166573662 test loss: nan\n",
      "[498/500] train loss: 1624.35548812624 test loss: nan\n",
      "[499/500] train loss: 1626.0663655598958 test loss: nan\n",
      "[500/500] train loss: 1635.842336503286 test loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1715.9096478836987\n"
     ]
    }
   ],
   "source": [
    "model, train_dataloader, test_dataloader = generate_model(4000, 28 * 28, 500, 1e-4, train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                   model_chain=None, learning_rate=1e-3,\n",
    "                   batch_size=32, rho=1, random_state=42, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d210dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(model, train_dataloader, \"chain1_train\", torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240ed5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(model, test_dataloader, \"chain1_test\", torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d426521",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChainedNoisyMNIST(\"chain1_train\")\n",
    "test_dataset = ChainedNoisyMNIST(\"chain1_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137d715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "[1/500] train loss: 18103.960441468254 test loss: 22492.314184804312\n",
      "[2/500] train loss: 18988.460968501986 test loss: nan\n",
      "[3/500] train loss: 19434.51688058036 test loss: nan\n",
      "[4/500] train loss: 20492.389322916668 test loss: nan\n",
      "[5/500] train loss: 18960.838820684523 test loss: nan\n",
      "[6/500] train loss: 19250.389105902777 test loss: nan\n",
      "[7/500] train loss: 18684.24045138889 test loss: nan\n",
      "[8/500] train loss: 17389.70535714286 test loss: nan\n",
      "[9/500] train loss: 18616.608243427578 test loss: nan\n",
      "[10/500] train loss: 25696.843889508928 test loss: nan\n",
      "[11/500] train loss: 22837.589440724205 test loss: 27074.008926342853\n",
      "[12/500] train loss: 18036.617544022818 test loss: nan\n",
      "[13/500] train loss: 20242.236839657737 test loss: nan\n",
      "[14/500] train loss: 18342.625635540673 test loss: nan\n",
      "[15/500] train loss: 20055.71702938988 test loss: nan\n",
      "[16/500] train loss: 23717.45380704365 test loss: nan\n",
      "[17/500] train loss: 23300.771902901786 test loss: nan\n",
      "[18/500] train loss: 20903.909226190477 test loss: nan\n",
      "Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[19/500] train loss: 23436.9937531002 test loss: nan\n",
      "[20/500] train loss: 13277.909900483632 test loss: nan\n",
      "[21/500] train loss: 9327.743954613095 test loss: 10132.244549346045\n",
      "[22/500] train loss: 8038.229360429067 test loss: nan\n",
      "[23/500] train loss: 7643.9179765004965 test loss: nan\n",
      "[24/500] train loss: 9293.978872147818 test loss: nan\n",
      "[25/500] train loss: 6947.966091579861 test loss: nan\n",
      "[26/500] train loss: 7279.068987165178 test loss: nan\n",
      "[27/500] train loss: 8149.726911272322 test loss: nan\n",
      "[28/500] train loss: 7107.298192584325 test loss: nan\n",
      "[29/500] train loss: 7035.182779947917 test loss: nan\n",
      "[30/500] train loss: 7038.75226702009 test loss: nan\n",
      "[31/500] train loss: 8333.37470548115 test loss: 9556.225855818191\n",
      "[32/500] train loss: 8328.144453745039 test loss: nan\n",
      "[33/500] train loss: 7663.580581907242 test loss: nan\n",
      "[34/500] train loss: 7655.715959821428 test loss: nan\n",
      "[35/500] train loss: 8880.978097098214 test loss: nan\n",
      "Epoch 00036: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[36/500] train loss: 7368.0625 test loss: nan\n",
      "[37/500] train loss: 5219.3225020151285 test loss: nan\n",
      "[38/500] train loss: 4397.377034505208 test loss: nan\n",
      "[39/500] train loss: 4242.899464440724 test loss: nan\n",
      "[40/500] train loss: 4167.667643229167 test loss: nan\n",
      "[41/500] train loss: 4113.061345176091 test loss: 5662.9008539461865\n",
      "[42/500] train loss: 4059.2698761470733 test loss: nan\n",
      "[43/500] train loss: 4021.243667844742 test loss: nan\n",
      "[44/500] train loss: 3983.686256045387 test loss: nan\n",
      "[45/500] train loss: 3940.0695025731648 test loss: nan\n",
      "[46/500] train loss: 3915.3799951946926 test loss: nan\n",
      "[47/500] train loss: 3873.929245721726 test loss: nan\n",
      "[48/500] train loss: 3819.5344044518847 test loss: nan\n",
      "[49/500] train loss: 3738.6115916418653 test loss: nan\n",
      "[50/500] train loss: 3687.7533598400296 test loss: nan\n",
      "[51/500] train loss: 3661.8060283358136 test loss: 4754.72315623128\n",
      "[52/500] train loss: 3665.0086030505954 test loss: nan\n",
      "[53/500] train loss: 3678.2594866071427 test loss: nan\n",
      "[54/500] train loss: 3712.531304253472 test loss: nan\n",
      "[55/500] train loss: 3988.3449745783732 test loss: nan\n",
      "[56/500] train loss: 4165.861138237848 test loss: nan\n",
      "[57/500] train loss: 3957.3260168650795 test loss: nan\n",
      "[58/500] train loss: 3933.8252611917164 test loss: nan\n",
      "[59/500] train loss: 3854.2005169580852 test loss: nan\n",
      "[60/500] train loss: 3588.57767547123 test loss: nan\n",
      "[61/500] train loss: 3532.5665593222966 test loss: 4475.0776819588655\n",
      "[62/500] train loss: 3543.2884695870534 test loss: nan\n",
      "[63/500] train loss: 3542.2676846943205 test loss: nan\n",
      "[64/500] train loss: 3545.0917929997518 test loss: nan\n",
      "[65/500] train loss: 3541.637181842138 test loss: nan\n",
      "[66/500] train loss: 3555.0821668836807 test loss: nan\n",
      "[67/500] train loss: 3586.002158513145 test loss: nan\n",
      "[68/500] train loss: 3651.8726089719744 test loss: nan\n",
      "[69/500] train loss: 3896.5379270523313 test loss: nan\n",
      "[70/500] train loss: 4212.048490978423 test loss: nan\n",
      "[71/500] train loss: 3988.6122930617557 test loss: 5619.5409298247805\n",
      "Epoch 00072: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[72/500] train loss: 3900.4861033606153 test loss: nan\n",
      "[73/500] train loss: 3151.8067743210568 test loss: nan\n",
      "[74/500] train loss: 2819.489511641245 test loss: nan\n",
      "[75/500] train loss: 2765.7425944010415 test loss: nan\n",
      "[76/500] train loss: 2732.289630223834 test loss: nan\n",
      "[77/500] train loss: 2709.4166802300347 test loss: nan\n",
      "[78/500] train loss: 2691.1110413566466 test loss: nan\n",
      "[79/500] train loss: 2676.0200776599704 test loss: nan\n",
      "[80/500] train loss: 2663.1531032986113 test loss: nan\n",
      "[81/500] train loss: 2651.9303133525545 test loss: 3056.059091001273\n",
      "[82/500] train loss: 2641.979263547867 test loss: nan\n",
      "[83/500] train loss: 2633.047072637649 test loss: nan\n",
      "[84/500] train loss: 2624.9537062872023 test loss: nan\n",
      "[85/500] train loss: 2617.565160357763 test loss: nan\n"
     ]
    }
   ],
   "source": [
    "model, train_dataloader, test_dataloader = generate_model(4000, 28 * 28, 500, 1e-4, train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                   model_chain=None, learning_rate=1e-3,\n",
    "                   batch_size=32, rho=1, random_state=35, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1da252",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(model, train_dataloader, \"chain2_train\", torch.device('cuda'))\n",
    "create_dataset(model, test_dataloader, \"chain2_test\", torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChainedNoisyMNIST(\"chain2_train\")\n",
    "test_dataset = ChainedNoisyMNIST(\"chain2_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_dataloader, test_dataloader = generate_model(4000, 28 * 28, 500, 1e-3, train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                   model_chain=None, learning_rate=1e-3,\n",
    "                   batch_size=32, rho=1, random_state=99, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(model, train_dataloader, \"chain3_train\", torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChainedNoisyMNIST(\"chain2_train\")\n",
    "test_dataset = ChainedNoisyMNIST(\"chain2_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95282f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_dataloader, test_dataloader = generate_model(4000, 28 * 28, 500, 1e-3, train_dataset=train_dataset, test_dataset=test_dataset,\n",
    "                   model_chain=None, learning_rate=1e-3,\n",
    "                   batch_size=32, rho=1, random_state=100, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e03b400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
